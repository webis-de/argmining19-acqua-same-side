{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Kate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read lemmatized data \n",
    "within_df =  pd.read_csv('lemmatized-data/within_test_gh_new.csv',encoding='utf-8', index_col='id')\n",
    "cross_df = pd.read_csv('lemmatized-data/cross_test_github.csv',encoding='utf-8', index_col='id')\n",
    "\n",
    "\n",
    "#uncomment the section below to read the unlemmatized data \n",
    "\n",
    "#within_df = pd.read_csv('data/same-side-classification/within-topic/within_test.csv',encoding='utf-8', index_col='id')\n",
    "#cross_df = pd.read_csv('data/same-side-classification/cross-topic/cross_test.csv',encoding='utf-8', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_df['iss_test'] = np.nan\n",
    "within_df['random_arg1'] = False\n",
    "within_df['random_arg2'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_df['iss_test'] = np.nan\n",
    "cross_df['iss_test'] = np.nan\n",
    "within_df['random_arg1'] = False\n",
    "within_df['random_arg2'] = False\n",
    "cross_df['random_arg1'] = False\n",
    "cross_df['random_arg2'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "def lemmatize_stemming(token, pos_tag):\n",
    "    stemmer = SnowballStemmer(\"english\") #pOrter, M. \"An algorithm for suffix stripping.\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(token, pos=pos_tag))\n",
    "\n",
    "def preprocess(text):\n",
    "    lemma = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        sentence = sentence.replace('\\n', ' ').strip()\n",
    "        tokens = [token for token in word_tokenize(sentence)]\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        \n",
    "        for idx in range(0,len(tokens)):\n",
    "            token = tokens[idx].lower()\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                wordnet_pos = get_wordnet_pos(pos_tags[idx][1])\n",
    "                l_ = lemmatize_stemming(token, wordnet_pos)\n",
    "                lemma.append(l_)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize data sets if needed; uncomment the section\n",
    "#within_df =  within_df.apply(get_lemma, axis=1)\n",
    "#cross_df =  cross_df.apply(get_lemma, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the files with \"good\" and \"bad\" words\n",
    "with open('../positive-words.txt', 'r') as f:\n",
    "    pos = [line.strip() for line in f]\n",
    "    \n",
    "with open('../negative-words.txt', 'r') as f:\n",
    "    neg = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize positive and negative words\n",
    "pos_lem = []\n",
    "for sent in pos:\n",
    "    pos_lem.append(' '.join(preprocess(sent)))\n",
    "neg_lem = []\n",
    "for sent in neg:\n",
    "    neg_lem.append(' '.join(preprocess(sent)))\n",
    "pos_lem = list(dict.fromkeys(pos_lem))\n",
    "neg_lem = list(dict.fromkeys(neg_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "count the amount of positive/negative words for each argument:\n",
    "if more positive -> argument gets the mark 1\n",
    "if more negative -> argument gets the mark 0\n",
    "if amount of pos/neg equal -> argument gets a random mark and a \"True\"-mark for the column \"random_arg1\" \n",
    "\n",
    "if both arguments have an equal mark -> column \"is_same_side\"=True, otherwise False\n",
    "'''\n",
    "\n",
    "\n",
    "def choose_your_side(row):\n",
    "    if pd.isnull(row['argument1_lemmas']):\n",
    "        arg1 = ' '\n",
    "    else:\n",
    "        arg1 = row['argument1_lemmas'].split()\n",
    "    \n",
    "    if pd.isnull(row['argument2_lemmas']):\n",
    "        arg2 = ' '\n",
    "    else:\n",
    "        arg2 = row['argument2_lemmas'].split()\n",
    "    \n",
    "    arg1_pos = [i for i in arg1 if i in pos_lem]\n",
    "    arg2_pos = [i for i in arg2 if i in pos_lem]\n",
    "    arg1_neg = [i for i in arg1 if i in neg_lem]\n",
    "    arg2_neg = [i for i in arg2 if i in neg_lem]\n",
    "    if len(arg1_pos) > len(arg1_neg):\n",
    "        arg1_label = 1\n",
    "    elif len(arg1_neg) > len(arg1_pos):\n",
    "        arg1_label = 0\n",
    "    else:\n",
    "        arg1_label = bool(random.getrandbits(1))\n",
    "        row['random_arg1'] = True\n",
    "    if len(arg2_pos) > len(arg2_neg):\n",
    "        arg2_label = 1\n",
    "    elif len(arg2_neg) > len(arg2_pos):\n",
    "        arg2_label = 0\n",
    "    else:\n",
    "        arg2_label = bool(random.getrandbits(1))\n",
    "        row['random_arg2'] = True\n",
    "    if arg1_label is arg2_label:\n",
    "        row['iss_test'] = True\n",
    "    else:\n",
    "        row['iss_test'] = False\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the function\n",
    "within_tested = within_df.apply(choose_your_side, axis = 1)\n",
    "cross_tested = cross_df.apply(choose_your_side, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.DataFrame(index = cross_tested.index, columns = ['label'])\n",
    "within = pd.DataFrame(index = within_tested.index, columns = ['label'])\n",
    "cross['label'] = cross_tested['iss_test']\n",
    "within['label'] = within_tested['iss_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predictions\n",
    "cross.to_csv(\"cross_test_predictions_rule-based.csv\")\n",
    "within.to_csv(\"within_test_new_predictions_rule-based.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
